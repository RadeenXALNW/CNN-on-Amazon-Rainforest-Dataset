# -*- coding: utf-8 -*-
"""Amazon Rainforest.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19BG-4kLGvqRSdLnZXUvOXJSQ3h1fqzrY
"""

!pip install PyDrive

import os
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

download = drive.CreateFile({'id': '1TiZIYfVm_wCXMsWos-Os0iWpebc7Hkyl'})
download.GetContentFile('train-jpg.tar')

!tar -xvf train-jpg.tar

from matplotlib import pyplot
from matplotlib.image import imread
# define location of dataset
folder = '/content/train-jpg/'
# plot first few images
for i in range(9):
	# define subplot
	pyplot.subplot(330 + 1 + i)
	# define filename
	filename = folder + 'train_' + str(i) + '.jpg'
	# load image pixels
	image = imread(filename)
	# plot raw pixel data
	pyplot.imshow(image)
# show the figure
pyplot.show()

download = drive.CreateFile({'id': '1kMdIymJdAMYJqm9ImvatIVMwmdydwBEe'})
download.GetContentFile('train_v2.csv')

#load and summarize the mapping file for the planet dataset

from pandas import read_csv
#load file as csv

filename="/content/train_v2.csv"
mapping_csv=read_csv(filename)
#summarize properties
print(mapping_csv.shape)
print(mapping_csv[:20])

from os import listdir
from numpy import zeros
from numpy import asarray
from numpy import savez_compressed
from pandas import read_csv
from keras.preprocessing.image import load_img
from keras.preprocessing.image import img_to_array

# create a mapping of tags to integers given the loaded mapping file
def create_tag_mapping(mapping_csv):
    #create a set of all known tags
    labels=set()
    for i in range (len(mapping_csv)):
        # convert spaced separated tags into an array of tags
        tags=mapping_csv['tags'][i].split(' ')
        # add tags to the set of known labels
        labels.update(tags)
    #convet set of labels to a list to list
    labels=list(labels)
    #order set alphabetically
    labels.sort()
    #dictionary that maps labels to integers, and the reverse

    labels_map={labels[i]:i for i in range(len(labels))}
    inv_labels_map={i:labels[i] for i in range(len(labels))}
    return labels_map,inv_labels_map


#create a mapping of filename to tags
def create_file_mapping(mapping_csv):
    mapping=dict()
    for i in range(len(mapping_csv)):
        name,tags=mapping_csv['image_name'][i],mapping_csv['tags'][i]
        mapping[name]=tags.split(' ')
    return mapping

#create a one hot encoding for one list of tags
def one_hot_encode(tags,mapping):
    #create empty vector
    encoding=zeros(len(mapping),dtype='uint8')
    #mark 1 for each tag in the vector
    for tag in tags:
        encoding[mapping[tag]]=1
    return encoding

#load all images into memory
def load_dataset(path,file_mapping,tag_mapping):
    photos,targets=list(),list()
    #enumerate files in the directory
    for filename in listdir(folder):
        #load image
        photo=load_img(path+filename,target_size=(32,32))
        #convert to numpy array
        photo=img_to_array(photo,dtype='uint8')
        #get tags
        tags=file_mapping[filename[:-4]]
        #one hot encode tags
        target=one_hot_encode(tags,tag_mapping)
        #store
        photos.append(photo)
        targets.append(target)
        
    X=asarray(photos,dtype='uint8')
    y=asarray(targets,dtype='uint8')
    return X,y


# load the mapping file
filename = "/content/train_v2.csv"
mapping_csv = read_csv(filename)
# create a mapping of tags to integers
tag_mapping, _ = create_tag_mapping(mapping_csv)
# create a mapping of filenames to tag lists
file_mapping = create_file_mapping(mapping_csv)

# load the jpeg images
folder='/content/train-jpg/'
X, y = load_dataset(folder, file_mapping, tag_mapping)
print(X.shape, y.shape)
# save both arrays to one file in compressed format
savez_compressed('/content/planet_data1.npz', X,y)

i = []
while True:
  i.append(i)

import sys
from numpy import load
from matplotlib import pyplot
from sklearn.model_selection import train_test_split
from keras import backend
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.layers import Conv2D
from keras.layers import MaxPooling2D
from keras.layers import Dense
from keras.layers import Flatten
from keras.models import Model
from keras.optimizers import Adam
from keras.applications.vgg16 import VGG16
# load train and test dataset
def load_dataset():
    # load dataset
    data = load('/content/planet_data1.npz')
    X, y = data['arr_0'], data['arr_1']
    # separate into train and test datasets
    trainX, testX, trainY, testY = train_test_split(X, y, test_size=0.3, random_state=1)
    print(trainX.shape, trainY.shape, testX.shape, testY.shape)
    return trainX, trainY, testX, testY

#calculate fbeta score for multi-class/label classification
def fbeta(y_true,y_pred,beta=2):
    #clip predictions
    y_pred=backend.clip(y_pred,0,1)
    #calculate elements
    tp=backend.sum(backend.round(backend.clip(y_true*y_pred,0,1)),axis=1)
    fp=backend.sum(backend.round(backend.clip(y_pred-y_true,0,1)),axis=1)
    fn=backend.sum(backend.round(backend.clip(y_true-y_pred,0,1)),axis=1)
    #calculate precision
    p=tp/(tp+fp+backend.epsilon())
    #calculate recall
    r=tp/(tp+fn+backend.epsilon())
    # calculate fbeta, averaged across each class
    bb = beta**2
    fbeta_score = backend.mean((1 + bb) * (p * r) / (bb * p + r + backend.epsilon()))
    return fbeta_score

#Define the model

# define cnn model
def define_model(in_shape=(32, 32, 3), out_shape=17):
    # load model
    model = VGG16(include_top=False, input_shape=in_shape)
    # mark loaded layers as not trainable
    for layer in model.layers:
        layer.trainable = False
    # allow last vgg block to be trainable
    model.get_layer('block5_conv1').trainable = True
    model.get_layer('block5_conv2').trainable = True
    model.get_layer('block5_conv3').trainable = True
    model.get_layer('block5_pool').trainable = True
    # add new classifier layers
    flat1 = Flatten()(model.layers[-1].output)
    class1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)
    output = Dense(out_shape, activation='sigmoid')(class1)
   # define new model
    model = Model(inputs=model.inputs, outputs=output)
    # compile model
    opt = Adam(lr=0.01)
    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=[fbeta])
    return model

#plot diagnostic learning curves
def summarize_diagnostic(history):
    #plot loss
    pyplot.subplot(211)
    pyplot.title('cross entropy loss')
    pyplot.plot(history.history['loss'], color='blue', label='train')
    pyplot.plot(history.history['val_loss'], color='orange', label='test')
    
    #plot accuracy
    pyplot.subplot(211)
    pyplot.title('cross entropy loss')
    pyplot.plot(history.history['fbeta'], color='blue', label='train')
    pyplot.plot(history.history['val_fbeta'], color='orange', label='test')
    # save plot to file
    filename = sys.argv[0].split('/')[-1]
    pyplot.savefig(filename + '_plot.png')
    pyplot.close()
    
# run the test harness for evaluating a model

def run_test_harness():
    #load dataset
    trainX,trainY,testX,testY=load_dataset()
    #create data generator
    train_datagen=ImageDataGenerator(featurewise_center=True,horizontal_flip=True,vertical_flip=True,rotation_range=90)
    test_datagen=ImageDataGenerator(featurewise_center=True)
    # specify imagenet mean values for centering
    train_datagen.mean = [123.68, 116.779, 103.939]
    test_datagen.mean = [123.68, 116.779, 103.939]
    #prepare iterations
    train_it=train_datagen.flow(trainX,trainY,batch_size=32)
    test_it=test_datagen.flow(testX,testY,batch_size=32)
    #define model
    model=define_model()
    # fit model
    history =model.fit_generator(train_it, steps_per_epoch=len(train_it),validation_data=test_it, validation_steps=len(test_it), epochs=40, verbose=0)
        
    
    #evaluate model
    loss,fbeta=model.evaluate_generator(test_it,steps=len(test_it),verbose=0)
    print('> loss=%.3f, fbeta=%.3f' % (loss, fbeta))
    # learning curves
    summarize_diagnostic(history)

#entry point,run the test harness
run_test_harness()

